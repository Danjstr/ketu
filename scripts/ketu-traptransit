#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import division, print_function
import os
import ketu
import h5py
import pandas as pd
from IPython.parallel import Client, require


@require(ketu, h5py)
def do_fit(args):
    lc, period, t0, depth, fn = args
    samples = ketu.k2.fit_traptransit(lc, period, t0, depth)
    with h5py.File(fn, "w") as f:
        f.create_dataset("samples", data=samples)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("candidate_file",
                        help="a CSV file listing the candidates")
    parser.add_argument("data_dir", help="the path to the data root")
    parser.add_argument("basis_file", help="the archive of PCA comps")
    parser.add_argument("out_dir", help="the output directory")
    args = parser.parse_args()

    # Initialize the pool.
    c = Client()
    c[:].push(dict(do_fit=do_fit))
    pool = c.load_balanced_view()
    jobs = []

    candidates = pd.read_csv(args.candidate_file)
    for id_, _ in candidates.groupby("kicid"):
        rows = candidates[candidates.kicid == id_]
        for i, (_, row) in enumerate(rows.iterrows()):
            epicid = row.kicid.split()[1]
            try:
                os.makedirs(args.out_dir)
            except os.error:
                pass

            light_curve_file = os.path.join(
                args.data_dir,
                "lightcurves/c1/{0}00000/{1}000/ktwo{2}-c01_lpd-lc.fits"
                .format(epicid[:4], epicid[4:6], epicid))

            # Set up the pipeline to load the data.
            pipe = ketu.k2.Data(cache=False)
            pipe = ketu.k2.Likelihood(pipe, cache=False)
            query = dict(
                basis_file=os.path.abspath(args.basis_file),
                light_curve_file=os.path.abspath(light_curve_file),
                nbasis=150,
                initial_time=1975.,
            )
            r = pipe.query(**query)
            lc = r.model_light_curves[0]
            fn = os.path.join(args.out_dir, epicid + "abc"[i] + ".h5")

            p = [lc, row.period, row.t0, row.depth, fn]
            jobs.append(pool.apply(do_fit, p))

    for job in jobs:
        job.get()
